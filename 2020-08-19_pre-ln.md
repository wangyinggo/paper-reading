# On Layer Normalization in the Transformer Architecture

## Motivation
To explore why the learning rate warm-up stage for transformer is essential.

## Pre-LN vs Post-LN
### Visualizations
![](figs/pre-ln_2020-08-27-15-20-20.png)

### Mathematical formulations
![](figs/pre-ln_2020-08-27-15-22-20.png)

## Understanding the transformer at initialization
![](figs/pre-ln_2020-08-27-15-25-15.png)

## Experimental results
![](figs/pre-ln_2020-08-27-15-26-23.png)

## Conclusion
- In the post-ln block, the expected gradients of the parameters near the output layer are large at initialization. This leads to an unstale training when using a large learning rate.

- The pre-ln block can be trained without the warm-up stage and converges much faster.