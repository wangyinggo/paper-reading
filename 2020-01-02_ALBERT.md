# Albert: A Lite BERT For Self-supervised Learning of Language Representations

## Abstract
- Two parameters-reduction techniques for BERT
    - Factorized embedding parameterization
    - Cross-layer parameter sharing
- A self-supervised loss that focuses on modeling inter-sentence coherence
- SOTA resultson the GLUE, RACE, and SQuAD bechmarks

## The Elements of ALBERT
Suppose that
- $E$: embedding size
- $L$: the number of encoder layers
- $H$: hidden size
- $V$: vocabulary size

### Factorized Embedding parameterization
- In BERT and its improvements, $E \equiv H$
- In ALBERT, $H \gg E$
- Using a factorization of the embedding parameters:
- $O(V \times H)$ to $O(V \times E + E \times H)$

### Cross-layer Parameter Sharing
Sharing all parameters
![2020-01-02_ALBERT2020-1-2-18-39-37.png](https://raw.githubusercontent.com/wangyinggo/paper-reading/master/pics/2020-01-02_ALBERT2020-1-2-18-39-37.png?token=AEFWIQUP244W4ST3C5DLKK26BXELE)

### Inter-sentence coherence loss
- Positive examples: two consecutive segments
- Negative examples: tow consecutive segments with order swapped

## Model Setup
![2020-01-02_ALBERT2020-1-2-19-49-44.png](https://raw.githubusercontent.com/wangyinggo/paper-reading/master/pics/2020-01-02_ALBERT2020-1-2-19-49-44.png?token=AEFWIQVLXMHBVSVX5UF6SMK6BXMSA)

## Experiments
![2020-01-02_ALBERT2020-1-2-20-49-40.png](https://raw.githubusercontent.com/wangyinggo/paper-reading/master/pics/2020-01-02_ALBERT2020-1-2-20-49-40.png)