# End-to-End ASR: From Supervised to Semi-Supervised Learning with Modern Architectures
## Abstract
- Pseudo-labeling for the semi-supervised training of ResNet, TDSï¼Œ Transformer for speech recognition with either CTC or Seq2Seq loss functions.
- Transformer superior alone, semi-supervision bridges the performance gap.
- A new SOTA
- the effect of different amounts of unlabeled audio

## Introduction
- E2E asr models are competitable
- Self-training can be quite effective.

![](figs/nihao_2020-08-04-17-36-11.png)

## Models
### Acoustic Models
- ResNet
- TDS
- Transformer

### Language Models
- n-gram
- GCNN
- Transformer

## Dataset
- LibriSpeech for supervised training
- LibriVox for pseudo-labeling - 53.8K hours

## Decoding
- Beam-search + LM + Rescoring
![](figs/e2easr_2020-08-04-17-51-21.png)
![](figs/e2easr2020-08-04-17-50-43.png)

## Experiments
![](figs/e2easr_2020-08-04-17-53-13.png)